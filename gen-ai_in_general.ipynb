{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4ef8561",
   "metadata": {},
   "source": [
    "# Разрез современного Генеративного машинного обучения\n",
    "\n",
    "**темы:** основные понятия и механизмы генеративного ИИ, обзор современных методов обучения генеративного ИИ, этика применения, споры и юридические аспекты \n",
    "\n",
    "**дата:** 02.02.2026\n",
    "\n",
    "**Автор:** Федоров Артем Максимович"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ea5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefc99eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a86a5f4",
   "metadata": {},
   "source": [
    "## Вероятностное моделирование \"in general\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcc88db",
   "metadata": {},
   "source": [
    "Самой популярной парадигмой современного машинного обучения является вероятностное моделирование, подразумевающее, что наблюдаемые данные порождаются некоторым законом распределения, существующим «в природе» – сам закон нам не известен, но мы видим, какие данные могут им порождаться по наблюдаемой обучающей выборке.\n",
    "\n",
    "Таким образом мы формализуем неопределённость и шум реального мира: вместо “жёстких” правил мы строим модель, какие наблюдения типичны и насколько. Где выделяются \"частоты\"  явления, там совершенно натуральным образом появляется и определение вероятности – чем чаще объекты (или их окрестности) встречаются в данных, тем выше должна быть их “масса” в распределении. Такая постановка делает методы:\n",
    "\n",
    "- более робастными (устойчивыми к шуму в данных)\n",
    "- интерпретируемыми (теперь модель выделяет вероятность \"правильного\" ответа – своей уверенности)\n",
    "- более гибкими (появляется конструктивный подход построения/модернизции моделей, учитывающих разные аспекты данных/доменов)\n",
    "\n",
    "\n",
    "> **Пример распределения изображений** из двух классов (котики и собачки). Здесь показано, что в некоторой области расположены два концентрированных облака точек, каждое из которых определено под соответствующий класс изображений; обратите внимание, что точки существуют не только в центрах облаков, но и в удалении или между ними – это так же валидные объекты, просто менее похожие на \"идеальных\" представителей (усредненных) классов. \n",
    "<p align=\"center\">\n",
    "  <img src=\"images/lecture_1/cats_n_dogs_distribution.png\" style=\"width:60%;\">\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4785f6",
   "metadata": {},
   "source": [
    "#### Понятие распределения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba017d3",
   "metadata": {},
   "source": [
    "Пусть мы так или иначе изучаем объекты $x$, принадлежащие (погруженные) в множество `носитель` $x \\in \\mathcal{X}$ $\\Rightarrow$ любой наблюдаемый нами объект существует внутри данного множетсва $\\mathcal{X}$. Потребность в явном выделении такого понятия как `носитель` может быть неочевидно, однако оно полезно, когда $\\mathcal{X}$ сам по себе сложный объект. Это может быть:\n",
    "\n",
    "| Носитель $\\mathcal{X}$ | Определение | Примеры использования в ML |\n",
    "| --- | --- | --- |\n",
    "| $\\mathcal{X} \\equiv \\mathbb{R}^d$ | Евклидово пространство признаков | эмбеддинги текста/изображений, векторизованные тензоры (матрицы) |\n",
    "| $\\mathcal{X} \\equiv \\mathbb{H}^d$ | Гиперболическое пространство признаков | эмбеддинги объектов, сохраняющие доменную иерархию |\n",
    "| $\\mathcal{X} \\equiv \\mathbb{S}^d_{++}$ | Множество симметричных положительно определённых матриц | ковариационные матрицы в сигналов/EEG/fMRI, матрицы рассеяния в CV, Riemannian ML в оптимизации |\n",
    "| $\\mathcal{X} \\equiv \\mathcal{G}$ | Множество графов | молекулярная генерация (генеративные GNN), генерация/дополнение knowledge graphs, моделирование социальных/транспортных сетей |\n",
    "| $\\mathcal{X} \\equiv \\mathcal{M}^n$ | Множество ранжировок/перестановок | learning-to-rank (поисковая выдача), рекомендательные системы (перестановки товаров), генерация упорядоченных списков при наличии комбинаторных ограничений |\n",
    "| $\\mathcal{X} \\subset \\Delta^{K-1}=\\left\\{p\\in\\mathbb{R}^K_{\\ge 0}:\\sum_{k=1}^K p_k=1\\right\\}$ | Вероятностный симплекс (векторы вероятностей) | тематическое моделирование (LDA/Dirichlet), смеси распределений, распределения над действиями в RL (policy) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759ffe2",
   "metadata": {},
   "source": [
    "Для фиксированного `носителя` $\\mathcal{X}$ мы можем ввести понятие `распределение` – закон, сопоставляющий каждой области $A \\subseteq \\mathcal{X}$ число $\\mathbb{P}(A) \\in [0, 1]$: вероятность того, что случайно выбранный объект $x$ окажется в этой области $$\\mathbb{P}(A) = \\mathbb{P}(\\textrm{Randomly Sampled:} \\,\\, x \\in A)$$\n",
    "\n",
    "Мы конструктивно определяем, в каких областях объекты существует и их появление/наблюдение там не редко (типично), а в каких областях $\\mathcal{X}$ концентрация минимальна (или вообще 0). У такого правила есть три базовых свойства:\n",
    "\n",
    "1) Неотрицательность: $\\mathbb{P}(A) \\leq 0$\n",
    "2) Нормировка: $\\mathbb{P}(\\mathcal{X})=1$ (вся масса распределения живёт на `носителе`)\n",
    "3) Аддитивность для непересекающихся областей: если $A \\cap B = \\varnothing$, то $\\mathbb{P}(A\\cup B)=\\mathbb{P}(A)+\\mathbb{P}(B)$\n",
    "\n",
    "Во многих задачах удобно описывать `распределение` не через вероятности областей напрямую, а через `плотность` $p(x)$ – мы требуем от `распределения` абсолютной непрерывности. Интуитивно $p(x)$ всюду на `носителе` $\\mathcal{X}$ определена и ее можно понимать как 'интенсивность массы' распределения в окрестности точки $x$ – с какой скоростью будет расти вероятность при увеличении маленькой области с центром в данной точке.\n",
    "Тогда вероятность того, что случайный объект $X$ попадёт в область $A \\subseteq \\mathcal{X}$, выражается интегрированием плотности по этой области:\n",
    "$$\\mathbb{P}(X \\in A) \\;=\\; \\int_{A} p(x)\\,dx \\quad\\mid\\quad \\int_{\\mathcal{X}} p(x)\\,dx \\;=\\; 1 $$\n",
    "\n",
    "> Пример бимодального (две вершины) абсолютно непрерывного распределения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "def mvn_pdf_grid(X, Y, mean, cov):\n",
    "    \"\"\"2D Gaussian density evaluated on a meshgrid (X, Y).\"\"\"\n",
    "    mean = np.asarray(mean, dtype=float).reshape(2,)\n",
    "    cov  = np.asarray(cov, dtype=float).reshape(2, 2)\n",
    "\n",
    "    pos  = np.stack([X, Y], axis=-1)               # (..., 2)\n",
    "    diff = pos - mean                              # (..., 2)\n",
    "\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    det_cov = np.linalg.det(cov)\n",
    "\n",
    "    quad = np.einsum(\"...i,ij,...j->...\", diff, inv_cov, diff)\n",
    "    norm = 1.0 / (2.0 * np.pi * np.sqrt(det_cov))\n",
    "    return norm * np.exp(-0.5 * quad)\n",
    "\n",
    "def mvn_pdf_points(P, mean, cov):\n",
    "    \"\"\"2D Gaussian density evaluated at points P of shape (N,2).\"\"\"\n",
    "    mean = np.asarray(mean, dtype=float).reshape(2,)\n",
    "    cov  = np.asarray(cov, dtype=float).reshape(2, 2)\n",
    "\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    diff = P - mean[None, :]\n",
    "\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    det_cov = np.linalg.det(cov)\n",
    "\n",
    "    quad = np.einsum(\"ni,ij,nj->n\", diff, inv_cov, diff)\n",
    "    norm = 1.0 / (2.0 * np.pi * np.sqrt(det_cov))\n",
    "    return norm * np.exp(-0.5 * quad)\n",
    "\n",
    "# --- Mixture parameters ---\n",
    "w1, w2 = 0.5, 0.5\n",
    "mu1 = np.array([-1.5, -1.0])\n",
    "mu2 = np.array([ 1.5,  1.2])\n",
    "\n",
    "# updated covariances (your values)\n",
    "cov1 = np.array([[0.6,  0.2],\n",
    "                 [0.2,  0.8]])\n",
    "\n",
    "cov2 = np.array([[1.12, -0.25],\n",
    "                 [-0.25, 1.15]])\n",
    "\n",
    "# --- Grid for surface ---\n",
    "xmin, xmax = -5, 5\n",
    "ymin, ymax = -5, 5\n",
    "n = 260\n",
    "\n",
    "x = np.linspace(xmin, xmax, n)\n",
    "y = np.linspace(ymin, ymax, n)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = w1 * mvn_pdf_grid(X, Y, mu1, cov1) + w2 * mvn_pdf_grid(X, Y, mu2, cov2)\n",
    "\n",
    "# --- Sampling 200 points from the mixture ---\n",
    "rng = np.random.default_rng(42)\n",
    "N = 200\n",
    "comp = rng.choice([0, 1], size=N, p=[w1, w2])\n",
    "\n",
    "samples = np.empty((N, 2))\n",
    "n1 = np.sum(comp == 0)\n",
    "n2 = N - n1\n",
    "\n",
    "# --- Plot ---\n",
    "fig = plt.figure(figsize=(11, 8), dpi=150)\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# surface (transparent)\n",
    "ax.plot_surface(\n",
    "    X, Y, Z,\n",
    "    rstride=1, cstride=1,\n",
    "    linewidth=0,\n",
    "    antialiased=True,\n",
    "    alpha=0.55,\n",
    "    shade=True\n",
    ")\n",
    "\n",
    "# wireframe overlay (grid visible on modes too)\n",
    "ax.plot_wireframe(\n",
    "    X, Y, Z,\n",
    "    rstride=10, cstride=10,\n",
    "    linewidth=0.6,\n",
    "    alpha=0.75\n",
    ")\n",
    "\n",
    "z0 = 0.0\n",
    "ax.contour(X, Y, Z, zdir=\"z\", offset=z0, levels=12, alpha=0.55)\n",
    "\n",
    "# labels / limits / view\n",
    "ax.set_title(\"Bimodal Gaussian Mixture Density (2 Gaussians)\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"p(x, y)\", labelpad=14)   # <- help z-label readability\n",
    "\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_zlim(z0, float(Z.max()) * 1.08)\n",
    "\n",
    "ax.view_init(elev=28, azim=-55)\n",
    "\n",
    "# cleaner panes\n",
    "ax.xaxis.pane.set_alpha(0.0)\n",
    "ax.yaxis.pane.set_alpha(0.0)\n",
    "ax.zaxis.pane.set_alpha(0.0)\n",
    "fig.subplots_adjust(left=0.02, right=0.86, bottom=0.02, top=0.92)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf1038",
   "metadata": {},
   "source": [
    "#### Случайные величины\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136e21a",
   "metadata": {},
   "source": [
    "\n",
    "**Подход теории вероятностей:** в основе лежит идея, что существует пространство элементарных исходов $\\Omega$ и закон вероятности на нём. Случайная величина $X\\in\\mathcal{X}$ — это отображение, которое каждому исходу $\\omega\\in\\Omega$ сопоставляет наблюдаемый объект $x=X(\\omega)\\in\\mathcal{X}$; тем самым $X$ «переносит» вероятностный закон с $\\Omega$ на пространство данных $\\mathcal{X}$, порождая распределение $P_X$ (или плотность $p_X$ в непрерывном случае): $\\mathbb{P}(X\\in A)=P_X(A)$ для областей $A\\subseteq\\mathcal{X}$, а при наличии плотности $P_X(A)=\\int_A p_X(x),dx$ и $\\int_{\\mathcal{X}} p_X(x),dx=1$.\n",
    "\n",
    "**Эмпирический подход:** если опустить формальное определение, то случайная величина — это переменная, значение которой не фиксировано заранее: при разных повторениях наблюдения мы получаем разные реализации $x$. Набор наблюдений ${x_i}_{i=1}^n$ можно рассматривать как сэмплы из некоторого распределения $P_X$ (часто пишут $x_i\\sim P_X$), которое и описывает, какие значения встречаются чаще, а какие реже.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912dc39a",
   "metadata": {},
   "source": [
    "#### Условное распределение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f889f",
   "metadata": {},
   "source": [
    "\n",
    "Пусть мы наблюдаем пару $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ как один сэмпл из некоторого *совместного распределения* $p(x,y)$ на носителе $\\mathcal{X}\\times\\mathcal{Y}$.\n",
    "\n",
    "Тогда *условное распределение* $p(y\\mid x)$ отвечает на вопрос:\n",
    "$$\n",
    "p(y\\mid x)\\ \\text{--- как распределён } y,\\ \\text{если наблюдение } x \\text{ зафиксировано.}\n",
    "$$\n",
    "\n",
    "Иначе говоря, при фиксированном $x$ функция $p(\\cdot\\mid x)$ задаёт распределение возможных значений $y$: она показывает, какие $y$ более типичны (имеют большую вероятность/плотность), а какие менее типичны *с учётом условия* $X=x$. Для каждого фиксированного $x$ это корректное распределение по $y$, то есть оно нормировано:\n",
    "$$\n",
    "\\int_{\\mathcal{Y}} p(y\\mid x),dy = 1\n",
    "\\quad\n",
    "\\text{(или } \\sum_{y\\in\\mathcal{Y}} p(y\\mid x)=1 \\text{ в дискретном случае).}\n",
    "$$\n",
    "\n",
    "Условное `распределение` и совместное связывает следующий закон:\n",
    "$$p(x, y) = p(y \\mid x) p(x) = p(x \\mid y) p(y) \\Rightarrow \\text{тогда для $p(x, y)$ распределения $p(x)$ и $p(y)$ являются маргинальными распределениями}$$\n",
    "\n",
    "Маргинализация `распределения` (или же закон выинтигрирования) позволяет получить из совместного `распределения` `маргинальное`  явным образом:\n",
    "$$p(x) = \\int_\\mathcal{Y} p(x, y) dy \\quad \\quad p(y) = \\int_\\mathcal{X} p(x, y) dx$$\n",
    "\n",
    "> Схематичный пример того, как выглядит бимодальное распределение объектов $x$, где каждая мода соответствует своему $y$ в совместном распределении $(x, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2942bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def kde_1d(x, grid, bandwidth=None):\n",
    "    \"\"\"\n",
    "    Simple Gaussian KDE (no SciPy).\n",
    "    x: (n,) samples\n",
    "    grid: (m,) evaluation points\n",
    "    bandwidth: if None -> Silverman's rule of thumb\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    n = x.size\n",
    "    if n < 2:\n",
    "        raise ValueError(\"Need at least 2 samples for KDE.\")\n",
    "\n",
    "    if bandwidth is None:\n",
    "        # Silverman's rule: h = 1.06 * std * n^{-1/5}\n",
    "        std = np.std(x, ddof=1)\n",
    "        bandwidth = 1.06 * std * (n ** (-1/5))\n",
    "        bandwidth = max(bandwidth, 1e-6)\n",
    "\n",
    "    u = (grid[:, None] - x[None, :]) / bandwidth\n",
    "    kernel = np.exp(-0.5 * u**2) / np.sqrt(2 * np.pi)\n",
    "    return kernel.mean(axis=1) / bandwidth\n",
    "\n",
    "# --- Synthetic data (replace with your own samples if needed) ---\n",
    "rng = np.random.default_rng(42)\n",
    "n1, n2 = 1200, 1200\n",
    "\n",
    "# class 1: slightly narrower\n",
    "x1 = np.r_[rng.normal(63.2, 2.1, int(0.9 * n1)),\n",
    "           rng.normal(60.2, 1.6, int(0.1 * n1))]\n",
    "\n",
    "# class 2: has a longer right tail\n",
    "x2 = np.r_[rng.normal(68.2, 2.0, int(0.8 * n2)),\n",
    "           rng.normal(73.0, 2.3, int(0.2 * n2))]\n",
    "\n",
    "x_all = np.r_[x1, x2]\n",
    "\n",
    "# --- KDE on a common grid ---\n",
    "xmin = min(x_all.min(), x1.min(), x2.min()) - 3\n",
    "xmax = max(x_all.max(), x1.max(), x2.max()) + 3\n",
    "grid = np.linspace(xmin, xmax, 600)\n",
    "\n",
    "d1 = kde_1d(x1, grid)\n",
    "d2 = kde_1d(x2, grid)\n",
    "d_all = kde_1d(x_all, grid)\n",
    "\n",
    "# --- Plot (matplotlib only) ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=150)\n",
    "\n",
    "# Colors similar to the reference figure\n",
    "c1 = \"#ff4d6d\"   # pink/red\n",
    "c2 = \"#4dabf7\"   # blue\n",
    "\n",
    "ax.fill_between(grid, 0, d1, alpha=0.35, color=c1)\n",
    "ax.plot(grid, d1, color=c1, linewidth=1.2, label=r\"$p(x \\mid y=\\mathrm{class}\\ 1)$\")\n",
    "\n",
    "ax.fill_between(grid, 0, d2, alpha=0.35, color=c2)\n",
    "ax.plot(grid, d2, color=c2, linewidth=1.2, label=r\"$p(x \\mid y=\\mathrm{class}\\ 2)$\")\n",
    "\n",
    "ax.plot(grid, d_all, color=\"black\", linestyle=(0, (4, 4)), linewidth=1.2, label=r\"$p(x)$\")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "ax.legend(loc=\"upper right\", frameon=True, fancybox=False, framealpha=1.0, edgecolor=\"black\")\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf77ee",
   "metadata": {},
   "source": [
    "#### Выборка\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1687e56",
   "metadata": {},
   "source": [
    "\n",
    "Выборка — это набор наблюдений, полученных из одного и того же неизвестного распределения данных в природе. Обычно предполагают, что элементы выборки независимы и одинаково распределены (i.i.d.), и записывают:\n",
    "$$\n",
    "x_1,\\dots,x_n \\sim p^*(x),\n",
    "$$\n",
    "где $p^*(x)$ — истинное (неизвестное) распределение. В задаче с разметкой наблюдают пары:\n",
    "$$\n",
    "(x_1,y_1),\\dots,(x_n,y_n) \\sim p^*(x,y).\n",
    "$$\n",
    "Задача обучения модели состоит в том, чтобы по выборке построить приближение $p_\\theta \\approx p^*$ (или соответствующие условные распределения).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3131fb",
   "metadata": {},
   "source": [
    "#### Моменты случайных величин: матожидание и дисперсия\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb00e9",
   "metadata": {},
   "source": [
    "\n",
    "Помимо самого распределения (массы/плотности), на практике часто удобно описывать случайную величину $X$ через её **моменты** — числовые характеристики, которые компактно суммируют “типичное значение” и “разброс”.\n",
    "\n",
    "**Математическое ожидание (mean)** — это среднее значение $X$ в смысле вероятностного закона:\n",
    "$$\n",
    "\\mathbb{E}[X] = \\int_{\\mathcal{X}} x\\,p(x)\\,dx\n",
    "\\quad \\text{(или } \\mathbb{E}[X]=\\sum_{x\\in\\mathcal{X}} x\\,p(x)\\text{ в дискретном случае).}\n",
    "$$\n",
    "Интуитивно $\\mathbb{E}[X]$ — это “центр массы” распределения.\n",
    "\n",
    "**Дисперсия (variance)** измеряет разброс значений вокруг среднего:\n",
    "$$\n",
    "\\mathrm{Var}(X)=\\mathbb{E}\\!\\left[(X-\\mathbb{E}[X])^2\\right]\n",
    "= \\int_{\\mathcal{X}} (x-\\mu)^2\\,p(x)\\,dx,\\quad \\mu=\\mathbb{E}[X].\n",
    "$$\n",
    "Часто используют также **стандартное отклонение** $\\sigma=\\sqrt{\\mathrm{Var}(X)}$, так как оно имеет те же единицы измерения, что и $X$.\n",
    "\n",
    "Для **векторных** данных $X\\in\\mathbb{R}^d$ аналогами являются средний вектор и ковариационная матрица:\n",
    "$$\n",
    "\\mu=\\mathbb{E}[X]\\in\\mathbb{R}^d,\\qquad\n",
    "\\Sigma=\\mathrm{Cov}(X)=\\mathbb{E}\\!\\left[(X-\\mu)(X-\\mu)^\\top\\right]\\in\\mathbb{R}^{d\\times d}.\n",
    "$$\n",
    "Диагональные элементы $\\Sigma$ — дисперсии отдельных координат, а внедиагональные — их взаимосвязь (корреляция/зависимость).\n",
    "\n",
    "Наконец, если у нас есть выборка $x_1,\\dots,x_n$, то моменты обычно оценивают эмпирически:\n",
    "$$\n",
    "\\hat\\mu=\\frac{1}{n}\\sum_{i=1}^n x_i,\\qquad\n",
    "\\widehat{\\mathrm{Var}}(X)=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\hat\\mu)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35330620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normal_pdf(x, mu, sigma):\n",
    "    return (1.0 / (np.sqrt(2*np.pi) * sigma)) * np.exp(-0.5 * ((x - mu) / sigma)**2)\n",
    "\n",
    "# параметры двух распределений: одинаковое матожидание, разная дисперсия\n",
    "mu = 0.0\n",
    "sigma1 = 1.0\n",
    "sigma2 = 2.0\n",
    "\n",
    "x = np.linspace(-8, 8, 800)\n",
    "p1 = normal_pdf(x, mu, sigma1)\n",
    "p2 = normal_pdf(x, mu, sigma2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5), dpi=150)\n",
    "\n",
    "ax.plot(x, p1, label=rf\"$p_1(x)$: $\\mathbb{{E}}[X]={mu}$, $\\mathrm{{Var}}(X)={sigma1**2}$\")\n",
    "ax.plot(x, p2, label=rf\"$p_2(x)$: $\\mathbb{{E}}[X]={mu}$, $\\mathrm{{Var}}(X)={sigma2**2}$\")\n",
    "\n",
    "# матожидание\n",
    "ax.axvline(mu, linewidth=1.6, label=rf\"$\\mu=\\mathbb{{E}}[X]$\")\n",
    "\n",
    "# интервалы +-1 sigma (для каждого распределения своим штрихом)\n",
    "ax.axvline(mu - sigma1, linestyle=\"--\", linewidth=1.2, label=rf\"$\\mu\\pm\\sigma_1$ (narrow)\")\n",
    "ax.axvline(mu + sigma1, linestyle=\"--\", linewidth=1.2)\n",
    "\n",
    "ax.axvline(mu - sigma2, linestyle=\":\", linewidth=1.4, label=rf\"$\\mu\\pm\\sigma_2$ (wide)\")\n",
    "ax.axvline(mu + sigma2, linestyle=\":\", linewidth=1.4)\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(r\"Mean $\\mathbb{E}[X]$ and spread via standard deviation $\\sigma=\\sqrt{\\mathrm{Var}(X)}$\")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.legend(loc=\"upper right\", frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dacfed3",
   "metadata": {},
   "source": [
    "## Дискриминативный vs. Генеративный ИИ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445aeb77",
   "metadata": {},
   "source": [
    "### Дискриминативный vs. Генеративный ИИ\n",
    "\n",
    "\n",
    "В вероятностном ML есть два **принципиально разных** режима работы с данными: `дискриминативный` и `генеративный`. Важно сразу зафиксировать: это **не** «классификация vs генерация изображений/текстов/графов (нужное подчеркнуть)». Классификация — лишь частный случай дискриминативного инференса, а генеративные модели часто решают *и* классификацию, и восстановление пропусков, и построение симуляторов — то есть работают как более общий инструмент моделирования данных.\n",
    "\n",
    "Далее (как и выше) будем считать:\n",
    "- $x\\in\\mathcal{X}$ — наблюдаемые признаки объекта (регрессоры),\n",
    "- $y\\in\\mathcal{Y}$ — скрытая/целевая характеристика (класс, число, метка, атрибут, условие)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619a0e5",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/lecture_1/discriminative_generative.png\" style=\"width:60%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980dbbd",
   "metadata": {},
   "source": [
    "#### Конструктивное сравнение двух подходов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175248c",
   "metadata": {},
   "source": [
    "\n",
    "**`Дискриминативная задача`** формулируется как моделирование условного распределения\n",
    "$\n",
    "p_\\theta(y\\mid x) \\Rightarrow\n",
    "$\n",
    "мы выучиваем «как распределён $y$, если $x$ зафиксирован». И на инференсе $x$ обязателен: без наблюдения признаков условное распределение просто не информативно для решения данной задачи.\n",
    "\n",
    "Здесь важно различать два уровня:\n",
    "1) **вероятностный выход** модели: она возвращает распределение/его параметры (например, вектор вероятностей классов, параметры гауссианы и т.п.);\n",
    "2) **предсказание** как детерминированное решение на основе распределения, например\n",
    "$$\n",
    "\\hat y(x)=\\arg\\max_{y\\in\\mathcal{Y}} p_\\theta(y\\mid x)\\quad\\text{(MAP-оценка)}, \\quad \\text{или} \\quad \\hat y(x)=\\mathbb{E}_\\theta[y\\mid x]\\quad\\text{(регрессия/байесовская точечная оценка).}\n",
    "$$\n",
    "\n",
    "\n",
    "> То есть дискриминативная модель по сути отвечает на вопрос: *«что можно утверждать про объект, если мы его уже наблюдаем?»*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d798e2",
   "metadata": {},
   "source": [
    "\n",
    "**`Генеративная задача`** ориентирована на моделирование распределения самих наблюдаемых данных. В зависимости от контекста моделируют:\n",
    "- безусловное $p_\\theta(x)$,\n",
    "- совместное $p_\\theta(x,y)$,\n",
    "- или условное $p_\\theta(x\\mid y)$ (контролируемая генерация).\n",
    "\n",
    "Ключевым критерием выступает умение модели **семплировать**/создавать новые наблюдения,\n",
    "$$\n",
    "x\\sim p_\\theta(x) \\qquad\\text{или}\\qquad x \\mid y \\sim p_\\theta(x\\mid y),\n",
    "$$\n",
    "такие, что они статистически согласованы с истинным распределением данных. Модель должна уметь **воспроизводить структуру множества объектов** и порождать новые экземпляры этой структуры. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcfc256",
   "metadata": {},
   "source": [
    "#### Связь подходов. Теорема Байеса\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44504e3",
   "metadata": {},
   "source": [
    "Ключевая связь между двумя парадигмами возникает, когда мы моделируем **совместный закон `распределения`** $p_\\theta(x,y)$. Тогда дискриминативное распределение получается как *производное* по формуле Байеса:\n",
    "$$\n",
    "p_\\theta(y\\mid x) \\;=\\; \\frac{p_\\theta(x, y)}{p_\\theta(x)} = \\frac{p_\\theta(x\\mid y)\\,p_\\theta(y)}{p_\\theta(x)},\n",
    "\\qquad\n",
    "p_\\theta(x)=\\underbrace{\\sum_{y\\in\\mathcal{Y}} p_\\theta(x\\mid y)\\,p_\\theta(y)}_{\\text{(дискретный \\(y\\))}} = \\underbrace{\\int p_\\theta(x\\mid y)\\,p_\\theta(y)\\,dy}_\\text{(непрерывный \\(y\\))}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4886d7",
   "metadata": {},
   "source": [
    "#### Сравнение обучения моделей двух подходов\n",
    "\n",
    "В дискриминативном обучении (при разметке) естественная цель — максимизация *условного* лог-правдоподобия:\n",
    "$$\n",
    "\\hat\\theta=\\arg\\max_\\theta \\sum_{i=1}^n \\log p_\\theta(y_i\\mid x_i),\n",
    "$$\n",
    "что в классификации эквивалентно минимизации кросс-энтропии (или логистический лосс в случае бинарной классификации), а в регрессии — взятию функции правдоподобия под выбранную модель шума (гауссовский $\\Rightarrow$ MSE, лапласовский $\\Rightarrow$ MAE и т.д.).\n",
    "\n",
    "В генеративном обучении мы исходим из гипотезы, что данные порождены некоторым неизвестным распределением $p^*(x)$, и хотим построить параметрическое приближение $p_\\theta(x)$, которое (в пределе бесконечных данных) согласуется с $p^*$ настолько, насколько это возможно в выбранном семействе моделей.\n",
    "\n",
    "Пусть $x_1,\\dots,x_n \\stackrel{i.i.d.}{\\sim} p^*(x)$. Конструктивная цель — максимизировать правдоподобие наблюдаемой выборки:\n",
    "$$\n",
    "\\hat\\theta_n=\\arg\\max_{\\theta\\in\\Theta}\\sum_{i=1}^n \\log p_\\theta(x_i)\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "\\arg\\min_{\\theta\\in\\Theta}\\; \\hat L_n(\\theta) = -\\frac1n\\sum_{i=1}^n \\log p_\\theta(x_i).\n",
    "$$\n",
    "\n",
    "При $n\\to\\infty$ по закону больших чисел $\\hat L_n(\\theta)\\to L(\\theta)$, где\n",
    "$$\n",
    "L(\\theta)=\\mathbb{E}_{x\\sim p^*}\\big[-\\log p_\\theta(x)\\big].\n",
    "$$\n",
    "\n",
    "Далее получаем стандартное разложение:\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\theta)\n",
    "&=\\mathbb{E}_{p^*}\\big[-\\log p_\\theta(x)\\big] \\\\\n",
    "&=\\mathbb{E}_{p^*}\\big[-\\log p^*(x)\\big] + \\mathbb{E}_{p^*}\\Big[\\log \\frac{p^*(x)}{p_\\theta(x)}\\Big] \\\\\n",
    "&=\\int_{\\mathcal{X}} p^*(x)\\,\\big[-\\log p^*(x)\\big]\\,dx \\;+\\; \\int_{\\mathcal{X}} p^*(x)\\,\\log\\!\\frac{p^*(x)}{p_\\theta(x)}\\,dx \\\\\n",
    "&=H(p^*)+\\mathrm{KL}\\!\\left(p^*\\|p_\\theta\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Поскольку $H(p^*)$ не зависит от $\\theta$, минимизация $L(\\theta)$ эквивалентна минимизации $\\mathrm{KL}(p^*\\|p_\\theta)$. Значит, максимизируя $\\sum_{i=1}^n \\log p_\\theta(x_i)$, мы подгоняем выучиваемое распределение $p_\\theta$ под истинное $p^*$ в KL-смысле — ровно то, что и является целью генеративного обучения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2d528",
   "metadata": {},
   "source": [
    "### Визуализация обучения Дискриминативного и Генеративного ИИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5db0ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Discriminative: Logistic regression via gradient descent (parameter path)\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "\n",
    "def logistic_regression_gd_path(\n",
    "    X1, X2,\n",
    "    lr=0.1, n_iters=200, l2=0.0, fit_intercept=True, seed=0,\n",
    "    lr_mode=\"constant\",            # <-- NEW: \"constant\" or \"warmup\"\n",
    "    warmup_power=2.0,              # <-- NEW: controls how slow the start is\n",
    "    lr_max=None                    # <-- NEW: if None, estimated automatically\n",
    "):\n",
    "    X1 = np.asarray(X1, dtype=float)\n",
    "    X2 = np.asarray(X2, dtype=float)\n",
    "    if X1.ndim != 2 or X2.ndim != 2:\n",
    "        raise ValueError(\"X1 and X2 must be 2D arrays of shape (n, d).\")\n",
    "    if X1.shape[1] != X2.shape[1]:\n",
    "        raise ValueError(\"X1 and X2 must have the same feature dimension d.\")\n",
    "\n",
    "    X = np.vstack([X1, X2])\n",
    "    y = np.concatenate([np.zeros(len(X1)), np.ones(len(X2))])\n",
    "\n",
    "    n, d = X.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    w = 0.01 * rng.standard_normal(d)\n",
    "    b = 0.0\n",
    "\n",
    "    def sigmoid(z):\n",
    "        z = np.clip(z, -50.0, 50.0)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def nll(w, b):\n",
    "        z = X @ w + (b if fit_intercept else 0.0)\n",
    "        p = sigmoid(z)\n",
    "        eps = 1e-12\n",
    "        loss = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n",
    "        if l2 > 0:\n",
    "            loss += 0.5 * l2 * np.sum(w * w)\n",
    "        return loss\n",
    "\n",
    "    # ---- NEW: choose lr_max automatically if warmup requested ----\n",
    "    if lr_mode == \"warmup\" and lr_max is None:\n",
    "        # Lipschitz upper bound using Frobenius norm\n",
    "        fro2 = np.sum(X * X)  # ||X||_F^2\n",
    "        L = 0.25 * (fro2 / n) + l2\n",
    "        lr_max = 0.9 / (L + 1e-12)\n",
    "\n",
    "    w_path = np.zeros((n_iters + 1, d))\n",
    "    b_path = np.zeros(n_iters + 1)\n",
    "    loss_path = np.zeros(n_iters + 1)\n",
    "\n",
    "    w_path[0] = w\n",
    "    b_path[0] = b\n",
    "    loss_path[0] = nll(w, b)\n",
    "\n",
    "    for t in range(1, n_iters + 1):\n",
    "        z = X @ w + (b if fit_intercept else 0.0)\n",
    "        p = sigmoid(z)\n",
    "\n",
    "        grad_w = (X.T @ (p - y)) / n\n",
    "        if l2 > 0:\n",
    "            grad_w += l2 * w\n",
    "\n",
    "        grad_b = np.mean(p - y) if fit_intercept else 0.0\n",
    "\n",
    "        # ---- NEW: step size schedule ----\n",
    "        if lr_mode == \"constant\":\n",
    "            step = lr\n",
    "        elif lr_mode == \"warmup\":\n",
    "            # step grows from ~0 to lr_max, making learning visibly gradual\n",
    "            frac = (t / n_iters) ** warmup_power\n",
    "            step = float(lr_max) * frac\n",
    "        else:\n",
    "            raise ValueError(\"lr_mode must be 'constant' or 'warmup'.\")\n",
    "\n",
    "        w -= step * grad_w\n",
    "        if fit_intercept:\n",
    "            b -= step * grad_b\n",
    "\n",
    "        w_path[t] = w\n",
    "        b_path[t] = b\n",
    "        loss_path[t] = nll(w, b)\n",
    "\n",
    "    return w_path, b_path, loss_path\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Generative: 2-Gaussian Mixture via EM + sampling\n",
    "# -----------------------------\n",
    "def sample_from_gmm(pi, mu, Sigma, n_samples=200, seed=0):\n",
    "    \"\"\"\n",
    "    Sample points from a 2-component Gaussian mixture.\n",
    "    Returns samples (n_samples,d) and component labels z (n_samples,).\n",
    "    \"\"\"\n",
    "    pi = np.asarray(pi, float)\n",
    "    mu = np.asarray(mu, float)\n",
    "    Sigma = np.asarray(Sigma, float)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    z = rng.choice(2, size=n_samples, p=pi)\n",
    "\n",
    "    d = mu.shape[1]\n",
    "    samples = np.zeros((n_samples, d), float)\n",
    "    for k in range(2):\n",
    "        idx = np.where(z == k)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        samples[idx] = rng.multivariate_normal(mean=mu[k], cov=Sigma[k], size=len(idx))\n",
    "    return samples, z\n",
    "\n",
    "\n",
    "def gmm2_em_and_sample(X1, X2, n_iters=100, tol=1e-6, reg_covar=1e-6, n_samples=200, seed=0):\n",
    "    \"\"\"\n",
    "    Fit a 2-component Gaussian Mixture Model using EM on the union of X1 and X2.\n",
    "    Uses X1 and X2 only for *initialization* (means/covariances/mixture weights).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X1, X2 : array-like\n",
    "        Two sets of points, shapes (n1, d), (n2, d).\n",
    "    n_iters : int\n",
    "        Max EM iterations.\n",
    "    tol : float\n",
    "        Convergence tolerance on log-likelihood improvement.\n",
    "    reg_covar : float\n",
    "        Diagonal regularizer added to covariances for numerical stability.\n",
    "    n_samples : int\n",
    "        Number of points to sample from the learned mixture.\n",
    "    seed : int\n",
    "        Random seed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params : dict\n",
    "        {\n",
    "          \"pi\": (2,),\n",
    "          \"mu\": (2, d),\n",
    "          \"Sigma\": (2, d, d),\n",
    "          \"loglik_path\": (T,)   # T <= n_iters\n",
    "        }\n",
    "    samples : np.ndarray\n",
    "        Shape (n_samples, d), sampled from the learned mixture.\n",
    "    responsibilities : np.ndarray\n",
    "        Shape (n, 2), final responsibilities r_{ik}.\n",
    "    \"\"\"\n",
    "    X1 = np.asarray(X1, dtype=float)\n",
    "    X2 = np.asarray(X2, dtype=float)\n",
    "    if X1.ndim != 2 or X2.ndim != 2:\n",
    "        raise ValueError(\"X1 and X2 must be 2D arrays of shape (n, d).\")\n",
    "    if X1.shape[1] != X2.shape[1]:\n",
    "        raise ValueError(\"X1 and X2 must have the same feature dimension d.\")\n",
    "\n",
    "    X = np.vstack([X1, X2])\n",
    "    n, d = X.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ----- Initialization (using the two sets) -----\n",
    "    mu = np.stack([X1.mean(axis=0), X2.mean(axis=0)], axis=0)  # (2, d)\n",
    "    def empirical_cov(A):\n",
    "        A = np.asarray(A, float)\n",
    "        if len(A) <= 1:\n",
    "            return np.eye(d)\n",
    "        C = np.cov(A.T, bias=False)\n",
    "        if C.ndim == 0:  # d=1 edge case\n",
    "            C = np.array([[float(C)]])\n",
    "        return C\n",
    "\n",
    "    Sigma = np.stack([empirical_cov(X1), empirical_cov(X2)], axis=0)  # (2, d, d)\n",
    "    Sigma += reg_covar * np.eye(d)[None, :, :]\n",
    "\n",
    "    pi = np.array([len(X1), len(X2)], dtype=float)\n",
    "    pi = pi / pi.sum()\n",
    "\n",
    "    # Helpers\n",
    "    def log_gaussian_pdf(X, m, S):\n",
    "        \"\"\"\n",
    "        log N(x | m, S) for each row in X. Returns shape (n,)\n",
    "        \"\"\"\n",
    "        # Cholesky for stability\n",
    "        L = np.linalg.cholesky(S)\n",
    "        # Solve (x-m) in whitened coordinates: v = L^{-1} (x-m)\n",
    "        Xm = X - m\n",
    "        v = np.linalg.solve(L, Xm.T)  # (d, n)\n",
    "        quad = np.sum(v * v, axis=0)  # (n,)\n",
    "        log_det = 2.0 * np.sum(np.log(np.diag(L)))\n",
    "        return -0.5 * (d * np.log(2.0 * np.pi) + log_det + quad)\n",
    "\n",
    "    def logsumexp(a, axis=1):\n",
    "        amax = np.max(a, axis=axis, keepdims=True)\n",
    "        return (amax + np.log(np.sum(np.exp(a - amax), axis=axis, keepdims=True))).squeeze(axis)\n",
    "\n",
    "    loglik_path = []\n",
    "    prev_ll = -np.inf\n",
    "\n",
    "    for it in range(n_iters):\n",
    "        # ----- E-step -----\n",
    "        # log r_ik proportional to log pi_k + log N(x_i | mu_k, Sigma_k)\n",
    "        log_r = np.zeros((n, 2))\n",
    "        for k in range(2):\n",
    "            log_r[:, k] = np.log(pi[k] + 1e-16) + log_gaussian_pdf(X, mu[k], Sigma[k])\n",
    "\n",
    "        ll = np.sum(logsumexp(log_r, axis=1))\n",
    "        loglik_path.append(ll)\n",
    "\n",
    "        # Normalize responsibilities\n",
    "        log_norm = logsumexp(log_r, axis=1)  # (n,)\n",
    "        r = np.exp(log_r - log_norm[:, None])  # (n,2)\n",
    "\n",
    "        # Convergence check\n",
    "        if it > 0 and abs(ll - prev_ll) < tol * (1.0 + abs(prev_ll)):\n",
    "            break\n",
    "        prev_ll = ll\n",
    "\n",
    "        # ----- M-step -----\n",
    "        Nk = r.sum(axis=0) + 1e-16  # (2,)\n",
    "        pi = Nk / n\n",
    "\n",
    "        # Update means\n",
    "        mu = (r.T @ X) / Nk[:, None]  # (2,d)\n",
    "\n",
    "        # Update covariances\n",
    "        Sigma_new = np.zeros((2, d, d))\n",
    "        for k in range(2):\n",
    "            Xm = X - mu[k]  # (n,d)\n",
    "            # Weighted covariance: sum_i r_ik (x_i-mu)(x_i-mu)^T / Nk\n",
    "            Sigma_new[k] = (Xm.T * r[:, k]) @ Xm / Nk[k]\n",
    "            Sigma_new[k] += reg_covar * np.eye(d)\n",
    "        Sigma = Sigma_new\n",
    "\n",
    "    # ----- Sampling from the learned mixture -----\n",
    "    # Choose component indices\n",
    "    z = rng.choice(2, size=n_samples, p=pi)\n",
    "    samples = np.zeros((n_samples, d))\n",
    "    for k in range(2):\n",
    "        idx = np.where(z == k)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        samples[idx] = rng.multivariate_normal(mean=mu[k], cov=Sigma[k], size=len(idx))\n",
    "\n",
    "    params = {\n",
    "        \"pi\": pi,\n",
    "        \"mu\": mu,\n",
    "        \"Sigma\": Sigma,\n",
    "        \"loglik_path\": np.array(loglik_path, dtype=float),\n",
    "    }\n",
    "    return params, samples, r\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: convenience (2D line from hyperplane)\n",
    "# -----------------------------\n",
    "def line_from_hyperplane_2d(w, b, x_min, x_max, n=200):\n",
    "    \"\"\"\n",
    "    For d=2, returns points on the decision boundary w^T x + b = 0:\n",
    "    y = -(w0/w1) x - b/w1\n",
    "    \"\"\"\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    if w.shape[0] != 2:\n",
    "        raise ValueError(\"This helper is only for 2D (w must have shape (2,)).\")\n",
    "    xs = np.linspace(x_min, x_max, n)\n",
    "    if abs(w[1]) < 1e-12:\n",
    "        # vertical line: w0 x + b = 0 -> x = -b/w0\n",
    "        x0 = -b / (w[0] + 1e-12)\n",
    "        ys = np.linspace(-1.0, 1.0, n)\n",
    "        xs = np.full_like(ys, x0)\n",
    "    else:\n",
    "        ys = -(w[0] * xs + b) / w[1]\n",
    "    return xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "246fb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: robustly unpack outputs\n",
    "# -----------------------------\n",
    "def _as_logreg_bundle(logreg_out):\n",
    "    \"\"\"\n",
    "    Accept either:\n",
    "      - dict with keys: X1, X2, w_path, b_path (loss_path optional)\n",
    "      - tuple: (w_path, b_path, loss_path, X1, X2) or (w_path, b_path, loss_path) + pass X1,X2 separately (not supported here)\n",
    "    \"\"\"\n",
    "    if isinstance(logreg_out, dict):\n",
    "        required = [\"X1\", \"X2\", \"w_path\", \"b_path\"]\n",
    "        for k in required:\n",
    "            if k not in logreg_out:\n",
    "                raise ValueError(f\"logreg_out dict must contain key '{k}'.\")\n",
    "        X1 = np.asarray(logreg_out[\"X1\"], float)\n",
    "        X2 = np.asarray(logreg_out[\"X2\"], float)\n",
    "        w_path = np.asarray(logreg_out[\"w_path\"], float)\n",
    "        b_path = np.asarray(logreg_out[\"b_path\"], float)\n",
    "        loss_path = np.asarray(logreg_out.get(\"loss_path\", []), float)\n",
    "        return X1, X2, w_path, b_path, loss_path\n",
    "\n",
    "    if isinstance(logreg_out, (tuple, list)) and len(logreg_out) >= 5:\n",
    "        w_path, b_path, loss_path, X1, X2 = logreg_out[:5]\n",
    "        return np.asarray(X1, float), np.asarray(X2, float), np.asarray(w_path, float), np.asarray(b_path, float), np.asarray(loss_path, float)\n",
    "\n",
    "    raise ValueError(\"Unsupported logreg_out format. Use dict or tuple (w_path,b_path,loss_path,X1,X2).\")\n",
    "\n",
    "\n",
    "def _as_gmm_bundle(gmm_out):\n",
    "    \"\"\"\n",
    "    Accept either:\n",
    "      - dict with keys: X1, X2, params, samples (responsibilities optional)\n",
    "      - tuple: (params, samples, responsibilities, X1, X2) or (params, samples, responsibilities) + pass X1,X2 separately (not supported here)\n",
    "    \"\"\"\n",
    "    if isinstance(gmm_out, dict):\n",
    "        required = [\"X1\", \"X2\", \"params\", \"samples\"]\n",
    "        for k in required:\n",
    "            if k not in gmm_out:\n",
    "                raise ValueError(f\"gmm_out dict must contain key '{k}'.\")\n",
    "        X1 = np.asarray(gmm_out[\"X1\"], float)\n",
    "        X2 = np.asarray(gmm_out[\"X2\"], float)\n",
    "        params = gmm_out[\"params\"]\n",
    "        samples = np.asarray(gmm_out[\"samples\"], float)\n",
    "        r = gmm_out.get(\"responsibilities\", None)\n",
    "        if r is not None:\n",
    "            r = np.asarray(r, float)\n",
    "        return X1, X2, params, samples, r\n",
    "\n",
    "    if isinstance(gmm_out, (tuple, list)) and len(gmm_out) >= 5:\n",
    "        params, samples, r, X1, X2 = gmm_out[:5]\n",
    "        return np.asarray(X1, float), np.asarray(X2, float), params, np.asarray(samples, float), (None if r is None else np.asarray(r, float))\n",
    "\n",
    "    raise ValueError(\"Unsupported gmm_out format. Use dict or tuple (params,samples,r,X1,X2).\")\n",
    "\n",
    "\n",
    "def _clamp_index(t, T):\n",
    "    if T <= 0:\n",
    "        return 0\n",
    "    return int(max(0, min(int(t), T - 1)))\n",
    "\n",
    "\n",
    "def _cov_ellipse_points(mu, Sigma, n=200, nsig=2.0):\n",
    "    \"\"\"\n",
    "    Return x,y points of an ellipse corresponding to nsig standard deviations\n",
    "    for a 2D Gaussian N(mu, Sigma).\n",
    "    \"\"\"\n",
    "    mu = np.asarray(mu, float).reshape(2,)\n",
    "    Sigma = np.asarray(Sigma, float).reshape(2, 2)\n",
    "\n",
    "    # Eigen-decomposition\n",
    "    vals, vecs = np.linalg.eigh(Sigma)\n",
    "    vals = np.maximum(vals, 1e-15)\n",
    "\n",
    "    # Parametric circle\n",
    "    theta = np.linspace(0, 2*np.pi, n)\n",
    "    circle = np.stack([np.cos(theta), np.sin(theta)], axis=0)  # (2, n)\n",
    "\n",
    "    # Scale circle to ellipse: vecs @ diag(sqrt(vals)) @ circle\n",
    "    A = vecs @ (np.diag(np.sqrt(vals)) * nsig)\n",
    "    ell = (A @ circle).T + mu[None, :]  # (n,2)\n",
    "    return ell[:, 0], ell[:, 1]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Plot: Logistic regression boundary at timestamp t\n",
    "# -----------------------------\n",
    "def plot_logreg_snapshot(logreg_out, t, ax=None, dims=(0, 1), padding=0.08, title=True):\n",
    "    \"\"\"\n",
    "    Plot a snapshot of logistic regression decision boundary at iteration t.\n",
    "    Expects d>=2 for visualization; uses dims=(i,j) projection.\n",
    "\n",
    "    logreg_out: dict with X1, X2, w_path, b_path (loss_path optional), or tuple.\n",
    "    \"\"\"\n",
    "    X1, X2, w_path, b_path, loss_path = _as_logreg_bundle(logreg_out)\n",
    "\n",
    "    if X1.shape[1] < 2:\n",
    "        raise ValueError(\"Need at least 2D points for a 2D boundary plot.\")\n",
    "    i, j = dims\n",
    "\n",
    "    T = w_path.shape[0]\n",
    "    t = _clamp_index(t, T)\n",
    "    w = w_path[t]\n",
    "    b = float(b_path[t])\n",
    "\n",
    "    # Project data and weights\n",
    "    X1p = X1[:, [i, j]]\n",
    "    X2p = X2[:, [i, j]]\n",
    "    wp = np.asarray([w[i], w[j]], float)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "    ax.scatter(X1p[:, 0], X1p[:, 1], s=18, label=\"X1 (y=0)\")\n",
    "    ax.scatter(X2p[:, 0], X2p[:, 1], s=18, label=\"X2 (y=1)\")\n",
    "\n",
    "    # Compute plot bounds\n",
    "    Xall = np.vstack([X1p, X2p])\n",
    "    xmin, ymin = Xall.min(axis=0)\n",
    "    xmax, ymax = Xall.max(axis=0)\n",
    "    dx, dy = xmax - xmin, ymax - ymin\n",
    "    xmin -= padding * (dx + 1e-12)\n",
    "    xmax += padding * (dx + 1e-12)\n",
    "    ymin -= padding * (dy + 1e-12)\n",
    "    ymax += padding * (dy + 1e-12)\n",
    "\n",
    "    # Decision boundary: wp[0]*x + wp[1]*y + b = 0\n",
    "    xs = np.linspace(xmin, xmax, 200)\n",
    "    if abs(wp[1]) < 1e-12:\n",
    "        # vertical line\n",
    "        x0 = -b / (wp[0] + 1e-12)\n",
    "        ax.plot([x0, x0], [ymin, ymax], linewidth=2)\n",
    "    else:\n",
    "        ys = -(wp[0] * xs + b) / wp[1]\n",
    "        ax.plot(xs, ys, linewidth=2)\n",
    "\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_xlabel(f\"x[{i}]\")\n",
    "    ax.set_ylabel(f\"x[{j}]\")\n",
    "\n",
    "    if title:\n",
    "        if loss_path.size == 0:\n",
    "            ax.set_title(f\"Logistic regression boundary\")\n",
    "        else:\n",
    "            # loss_path might be length T, but be robust\n",
    "            lt = loss_path[t] if t < len(loss_path) else np.nan\n",
    "            ax.set_title(f\"Logistic regression boundary\")\n",
    "\n",
    "    ax.legend(frameon=False)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    return ax\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Plot: GMM snapshot at timestamp t\n",
    "# -----------------------------\n",
    "def plot_gmm_snapshot(\n",
    "    gmm_out, t, ax=None, dims=(0, 1), padding=0.08, title=True,\n",
    "    show_samples=True, ellipse_nsig=(1.0, 2.0),\n",
    "    n_samples=250, sample_seed=0, color_samples=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a 2-component GMM snapshot.\n",
    "    FIX: samples are generated from the parameters at the selected timestamp t.\n",
    "\n",
    "    If params has history: use params at iter t.\n",
    "    Otherwise: use final params.\n",
    "    \"\"\"\n",
    "    X1, X2, params, _samples_unused, r = _as_gmm_bundle(gmm_out)\n",
    "\n",
    "    if X1.shape[1] < 2:\n",
    "        raise ValueError(\"Need at least 2D points for a 2D GMM plot.\")\n",
    "    i, j = dims\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "    # --- Scatter data and capture their colors ---\n",
    "    X1p = X1[:, [i, j]]\n",
    "    X2p = X2[:, [i, j]]\n",
    "    sc1 = ax.scatter(X1p[:, 0], X1p[:, 1], s=18, label=\"X1\")\n",
    "    sc2 = ax.scatter(X2p[:, 0], X2p[:, 1], s=18, label=\"X2\")\n",
    "\n",
    "    def _pick_color(sc):\n",
    "        fc = sc.get_facecolor()\n",
    "        if fc is not None and len(fc) > 0:\n",
    "            return fc[0]\n",
    "        ec = sc.get_edgecolor()\n",
    "        if ec is not None and len(ec) > 0:\n",
    "            return ec[0]\n",
    "        return None\n",
    "\n",
    "    col_X1 = _pick_color(sc1)\n",
    "    col_X2 = _pick_color(sc2)\n",
    "\n",
    "    # --- Choose which parameters to plot ---\n",
    "    if \"history\" in params and params[\"history\"] is not None:\n",
    "        hist = params[\"history\"]\n",
    "        pi_path = np.asarray(hist[\"pi_path\"], float)\n",
    "        mu_path = np.asarray(hist[\"mu_path\"], float)\n",
    "        Sig_path = np.asarray(hist[\"Sigma_path\"], float)\n",
    "        T = pi_path.shape[0]\n",
    "        tt = _clamp_index(t, T)\n",
    "        pi = pi_path[tt]\n",
    "        mu = mu_path[tt]\n",
    "        Sigma = Sig_path[tt]\n",
    "        used_t = tt\n",
    "    else:\n",
    "        pi = np.asarray(params[\"pi\"], float)\n",
    "        mu = np.asarray(params[\"mu\"], float)\n",
    "        Sigma = np.asarray(params[\"Sigma\"], float)\n",
    "        used_t = None\n",
    "        tt = 0  # for deterministic seed below\n",
    "\n",
    "    # --- Stabilize component<->dataset color mapping by mean proximity ---\n",
    "    m1 = X1.mean(axis=0)\n",
    "    m2 = X2.mean(axis=0)\n",
    "    c_01 = np.linalg.norm(mu[0] - m1) + np.linalg.norm(mu[1] - m2)\n",
    "    c_10 = np.linalg.norm(mu[0] - m2) + np.linalg.norm(mu[1] - m1)\n",
    "    if c_01 <= c_10:\n",
    "        comp_color = [col_X1, col_X2]\n",
    "    else:\n",
    "        comp_color = [col_X2, col_X1]\n",
    "\n",
    "    # --- Plot means and ellipses with fixed colors ---\n",
    "    for k in range(2):\n",
    "        color = comp_color[k]\n",
    "        muk = mu[k, [i, j]]\n",
    "        Sigk = Sigma[k][np.ix_([i, j], [i, j])]\n",
    "\n",
    "        ax.scatter([muk[0]], [muk[1]], s=60, marker=\"x\", linewidths=2, color=color)\n",
    "\n",
    "        for ns in ellipse_nsig:\n",
    "            ex, ey = _cov_ellipse_points(muk, Sigk, nsig=float(ns))\n",
    "            ax.plot(ex, ey, linewidth=1.5, color=color)\n",
    "\n",
    "    # --- FIX: sample from CURRENT (pi,mu,Sigma) at timestamp t ---\n",
    "    if show_samples and n_samples > 0:\n",
    "        # deterministic per-timestamp seed so samples don't jump around\n",
    "        seed_t = int(sample_seed) + 10_000 * int(tt)\n",
    "        samples_t, z_t = sample_from_gmm(pi, mu, Sigma, n_samples=n_samples, seed=seed_t)\n",
    "        sp = samples_t[:, [i, j]]\n",
    "\n",
    "        if color_samples:\n",
    "            # color samples by component, using the same fixed colors as ellipses\n",
    "            for k in range(2):\n",
    "                idx = np.where(z_t == k)[0]\n",
    "                if len(idx) == 0:\n",
    "                    continue\n",
    "                ax.scatter(sp[idx, 0], sp[idx, 1], s=10, marker=\".\", color=comp_color[k], alpha=0.8)\n",
    "        else:\n",
    "            ax.scatter(sp[:, 0], sp[:, 1], s=10, marker=\".\", label=\"samples\")\n",
    "\n",
    "    # --- Bounds ---\n",
    "    Xall = np.vstack([X1p, X2p])\n",
    "    xmin, ymin = Xall.min(axis=0)\n",
    "    xmax, ymax = Xall.max(axis=0)\n",
    "    dx, dy = xmax - xmin, ymax - ymin\n",
    "    xmin -= padding * (dx + 1e-12)\n",
    "    xmax += padding * (dx + 1e-12)\n",
    "    ymin -= padding * (dy + 1e-12)\n",
    "    ymax += padding * (dy + 1e-12)\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    ax.set_xlabel(f\"x[{i}]\")\n",
    "    ax.set_ylabel(f\"x[{j}]\")\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "\n",
    "    if title:\n",
    "        llp = params.get(\"loglik_path\", None)\n",
    "        if llp is not None and len(llp) > 0:\n",
    "            llp = np.asarray(llp, float)\n",
    "            t_ll = _clamp_index(t, len(llp))\n",
    "            msg = f\"2-GMM (EM); loglik[t={t_ll}]={llp[t_ll]:.2f}\"\n",
    "            msg += f\"; params @ iter {used_t}\" if used_t is not None else \"; params = final\"\n",
    "            ax.set_title(msg)\n",
    "        else:\n",
    "            ax.set_title(\"2-GMM (EM) snapshot\")\n",
    "\n",
    "    ax.legend(frameon=False)\n",
    "    return ax\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Plot both paradigms in one row\n",
    "# -----------------------------\n",
    "def plot_two_paradigms_row(logreg_out, gmm_out, t, dims=(0, 1), figsize=(12, 5), share_limits=False):\n",
    "    \"\"\"\n",
    "    Draw two plots in a single row:\n",
    "      - Left: discriminative logistic regression boundary @ t\n",
    "      - Right: generative GMM fit @ t (or final if history absent)\n",
    "\n",
    "    If share_limits=True, both axes use the same x/y limits (based on combined data).\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "    ax0 = plot_logreg_snapshot(logreg_out, t=t, ax=axes[0], dims=dims, title=True)\n",
    "    ax1 = plot_gmm_snapshot(gmm_out, t=t, ax=axes[1], dims=dims, title=True)\n",
    "\n",
    "    if share_limits:\n",
    "        # Compute combined limits\n",
    "        X1, X2, _, _, _ = _as_logreg_bundle(logreg_out)\n",
    "        Xall = np.vstack([X1[:, list(dims)], X2[:, list(dims)]])\n",
    "        xmin, ymin = Xall.min(axis=0)\n",
    "        xmax, ymax = Xall.max(axis=0)\n",
    "        dx, dy = xmax - xmin, ymax - ymin\n",
    "        pad = 0.08\n",
    "        xmin -= pad * (dx + 1e-12); xmax += pad * (dx + 1e-12)\n",
    "        ymin -= pad * (dy + 1e-12); ymax += pad * (dy + 1e-12)\n",
    "        ax0.set_xlim(xmin, xmax); ax0.set_ylim(ymin, ymax)\n",
    "        ax1.set_xlim(xmin, xmax); ax1.set_ylim(ymin, ymax)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d493c3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37de1cacff9487cb35954474dcf5984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(button_style='primary', description='Пересчитать', icon='refresh', style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset generator (as requested)\n",
    "# -----------------------------\n",
    "def create_dataset(n1=200, n2=200, d=2, separation=2.5, cov_scale=0.8, rotate_deg=25.0, seed=0):\n",
    "    \"\"\"\n",
    "    Generate two point clouds X1 and X2 in R^d (default d=2) with controllable overlap.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X1 : (n1, d)\n",
    "    X2 : (n2, d)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    if d != 2:\n",
    "        # For simplicity of visualization, we stick to d=2 by default.\n",
    "        # You can generalize means/covariances for any d if needed.\n",
    "        raise ValueError(\"This create_dataset is set for d=2 to support 2D visualization.\")\n",
    "\n",
    "    # Base covariance\n",
    "    S0 = cov_scale * np.array([[1.0, 0.6],\n",
    "                               [0.6, 1.4]])\n",
    "\n",
    "    # Rotation\n",
    "    theta = np.deg2rad(rotate_deg)\n",
    "    R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                  [np.sin(theta),  np.cos(theta)]])\n",
    "    S1 = R @ S0 @ R.T\n",
    "    S2 = R.T @ S0 @ R  # slightly different orientation\n",
    "\n",
    "    # Means\n",
    "    mu1 = np.array([-separation/2, 0.0])\n",
    "    mu2 = np.array([+separation/2, 0.0])\n",
    "\n",
    "    X1 = rng.multivariate_normal(mu1, S1, size=n1)\n",
    "    X2 = rng.multivariate_normal(mu2, S2, size=n2)\n",
    "    return X1, X2\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# EM for 2-GMM WITH history (so sliders truly move parameters)\n",
    "# -----------------------------\n",
    "def gmm2_em_and_sample_history(\n",
    "    X1, X2,\n",
    "    n_iters=80, tol=1e-6, reg_covar=1e-6, n_samples=200, seed=0,\n",
    "    init=\"collapsed\",          # <-- NEW: \"from_sets\" (fast), \"random\", \"collapsed\" (slow+pedagogical)\n",
    "    damping=0.25,              # <-- NEW: alpha in (0,1], smaller => slower\n",
    "    early_stop=False           # <-- NEW: for demos keep False to run full n_iters\n",
    "):\n",
    "    X1 = np.asarray(X1, float)\n",
    "    X2 = np.asarray(X2, float)\n",
    "    X = np.vstack([X1, X2])\n",
    "    n, d = X.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ---------- Initialization ----------\n",
    "    if init == \"from_sets\":\n",
    "        mu = np.stack([X1.mean(axis=0), X2.mean(axis=0)], axis=0)\n",
    "\n",
    "        def empirical_cov(A):\n",
    "            if len(A) <= 1:\n",
    "                return np.eye(d)\n",
    "            C = np.cov(A.T, bias=False)\n",
    "            if C.ndim == 0:\n",
    "                C = np.array([[float(C)]])\n",
    "            return C\n",
    "\n",
    "        Sigma = np.stack([empirical_cov(X1), empirical_cov(X2)], axis=0)\n",
    "        Sigma += reg_covar * np.eye(d)[None, :, :]\n",
    "        pi = np.array([len(X1), len(X2)], float)\n",
    "        pi = pi / pi.sum()\n",
    "\n",
    "    elif init == \"random\":\n",
    "        idx = rng.choice(n, size=2, replace=False)\n",
    "        mu = X[idx].copy()\n",
    "        C = np.cov(X.T, bias=False)\n",
    "        if C.ndim == 0:\n",
    "            C = np.array([[float(C)]])\n",
    "        Sigma = np.stack([C, C], axis=0) + reg_covar * np.eye(d)[None, :, :]\n",
    "        pi = np.array([0.5, 0.5], float)\n",
    "\n",
    "    elif init == \"collapsed\":\n",
    "        # both components start near the global mean -> EM must \"split\" them gradually\n",
    "        m = X.mean(axis=0)\n",
    "        jitter = 0.05 * rng.standard_normal((2, d))\n",
    "        mu = np.stack([m, m], axis=0) + jitter\n",
    "\n",
    "        C = np.cov(X.T, bias=False)\n",
    "        if C.ndim == 0:\n",
    "            C = np.array([[float(C)]])\n",
    "        Sigma = np.stack([C, C], axis=0) + reg_covar * np.eye(d)[None, :, :]\n",
    "        pi = np.array([0.5, 0.5], float)\n",
    "\n",
    "    elif init == \"pca_split\":\n",
    "        # Start both components near global mean but separated along the principal direction.\n",
    "        m = X.mean(axis=0)\n",
    "\n",
    "        C = np.cov(X.T, bias=False)\n",
    "        if C.ndim == 0:\n",
    "            C = np.array([[float(C)]])\n",
    "\n",
    "        # principal eigenvector\n",
    "        vals, vecs = np.linalg.eigh(C)\n",
    "        v = vecs[:, -1]                       # (d,)\n",
    "        s = np.sqrt(max(vals[-1], 1e-12))     # scale of spread along principal axis\n",
    "\n",
    "        # delta controls how \"far\" the initial means are; smaller -> slower but still breaks symmetry\n",
    "        split_scale = 0.35                    # try 0.25..0.6\n",
    "        delta = split_scale * s\n",
    "\n",
    "        mu = np.stack([m - delta * v, m + delta * v], axis=0)\n",
    "\n",
    "        # same covariance for both components at start (uninformative)\n",
    "        Sigma = np.stack([C, C], axis=0) + reg_covar * np.eye(d)[None, :, :]\n",
    "\n",
    "        # slightly asymmetric mixing weights to break symmetry further\n",
    "        pi = np.array([0.55, 0.45], float)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"init must be one of: 'from_sets', 'random', 'collapsed'.\")\n",
    "\n",
    "    # ---------- Helpers ----------\n",
    "    def log_gaussian_pdf(X, m, S):\n",
    "        L = np.linalg.cholesky(S)\n",
    "        Xm = X - m\n",
    "        v = np.linalg.solve(L, Xm.T)\n",
    "        quad = np.sum(v * v, axis=0)\n",
    "        log_det = 2.0 * np.sum(np.log(np.diag(L)))\n",
    "        return -0.5 * (d * np.log(2.0 * np.pi) + log_det + quad)\n",
    "\n",
    "    def logsumexp(a, axis=1):\n",
    "        amax = np.max(a, axis=axis, keepdims=True)\n",
    "        return (amax + np.log(np.sum(np.exp(a - amax), axis=axis, keepdims=True))).squeeze(axis)\n",
    "\n",
    "    # ---------- History ----------\n",
    "    loglik_path = []\n",
    "    pi_path, mu_path, Sigma_path = [], [], []\n",
    "\n",
    "    prev_ll = -np.inf\n",
    "\n",
    "    for it in range(n_iters):\n",
    "        # store snapshot (so slider can show evolution)\n",
    "        pi_path.append(pi.copy())\n",
    "        mu_path.append(mu.copy())\n",
    "        Sigma_path.append(Sigma.copy())\n",
    "\n",
    "        # E-step\n",
    "        log_r = np.zeros((n, 2))\n",
    "        for k in range(2):\n",
    "            log_r[:, k] = np.log(pi[k] + 1e-16) + log_gaussian_pdf(X, mu[k], Sigma[k])\n",
    "\n",
    "        ll = float(np.sum(logsumexp(log_r, axis=1)))\n",
    "        loglik_path.append(ll)\n",
    "\n",
    "        log_norm = logsumexp(log_r, axis=1)\n",
    "        r = np.exp(log_r - log_norm[:, None])\n",
    "\n",
    "        # optional early stop (disabled by default for pedagogy)\n",
    "        if early_stop and it > 0 and abs(ll - prev_ll) < tol * (1.0 + abs(prev_ll)):\n",
    "            break\n",
    "        prev_ll = ll\n",
    "\n",
    "        # M-step (compute \"new\" params)\n",
    "        Nk = r.sum(axis=0) + 1e-16\n",
    "        pi_new = Nk / n\n",
    "        mu_new = (r.T @ X) / Nk[:, None]\n",
    "\n",
    "        Sigma_new = np.zeros((2, d, d))\n",
    "        for k in range(2):\n",
    "            Xm = X - mu_new[k]\n",
    "            Sigma_new[k] = (Xm.T * r[:, k]) @ Xm / Nk[k]\n",
    "            Sigma_new[k] += reg_covar * np.eye(d)\n",
    "\n",
    "        # ---- NEW: damped update ----\n",
    "        a = float(damping)\n",
    "        pi = (1 - a) * pi + a * pi_new\n",
    "        pi = pi / np.sum(pi)  # renormalize\n",
    "\n",
    "        mu = (1 - a) * mu + a * mu_new\n",
    "        Sigma = (1 - a) * Sigma + a * Sigma_new\n",
    "\n",
    "    # final snapshot\n",
    "    pi_path.append(pi.copy())\n",
    "    mu_path.append(mu.copy())\n",
    "    Sigma_path.append(Sigma.copy())\n",
    "\n",
    "    # Sampling from final mixture\n",
    "    z = rng.choice(2, size=n_samples, p=pi)\n",
    "    samples = np.zeros((n_samples, d))\n",
    "    for k in range(2):\n",
    "        idx = np.where(z == k)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        samples[idx] = rng.multivariate_normal(mu[k], Sigma[k], size=len(idx))\n",
    "\n",
    "    params = {\n",
    "        \"pi\": pi,\n",
    "        \"mu\": mu,\n",
    "        \"Sigma\": Sigma,\n",
    "        \"loglik_path\": np.array(loglik_path, float),\n",
    "        \"history\": {\n",
    "            \"pi_path\": np.array(pi_path, float),\n",
    "            \"mu_path\": np.array(mu_path, float),\n",
    "            \"Sigma_path\": np.array(Sigma_path, float),\n",
    "        }\n",
    "    }\n",
    "    return params, samples, r\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Widget tool\n",
    "# -----------------------------\n",
    "def build_discriminative_vs_generative_widget(\n",
    "    *,\n",
    "    dataset_kwargs=None,\n",
    "    logreg_kwargs=None,\n",
    "    gmm_kwargs=None,\n",
    "    dims=(0, 1),\n",
    "    share_limits=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates an ipywidgets UI:\n",
    "      - Button \"Пересчитать\"\n",
    "      - Slider for LR iteration\n",
    "      - Slider for EM iteration\n",
    "      - 2 plots in a row (discriminative boundary + generative GMM)\n",
    "\n",
    "    Notes:\n",
    "      - Requires logistic_regression_gd_path, plot_logreg_snapshot, plot_gmm_snapshot already defined.\n",
    "      - Uses gmm2_em_and_sample_history defined above.\n",
    "    \"\"\"\n",
    "    if dataset_kwargs is None:\n",
    "        dataset_kwargs = dict(n1=100, n2=100, d=2, separation=2.6, cov_scale=0.9, rotate_deg=20.0, seed=0)\n",
    "    if logreg_kwargs is None:\n",
    "        logreg_kwargs = dict(\n",
    "            lr=0.1,\n",
    "            n_iters=20,\n",
    "            l2=1e-2,\n",
    "            fit_intercept=True,\n",
    "            seed=0,\n",
    "            lr_mode=\"warmup\",\n",
    "            warmup_power=3.0,\n",
    "            lr_max=None\n",
    "        )\n",
    "    if gmm_kwargs is None:\n",
    "        gmm_kwargs = gmm_kwargs = dict(\n",
    "            n_iters=100,\n",
    "            tol=1e-6,\n",
    "            reg_covar=1e-6,\n",
    "            n_samples=250,\n",
    "            seed=0,\n",
    "            init=\"pca_split\",\n",
    "            damping=0.20,\n",
    "            early_stop=False\n",
    "        )\n",
    "\n",
    "    # UI widgets\n",
    "    btn = widgets.Button(description=\"Пересчитать\", button_style=\"primary\", icon=\"refresh\")\n",
    "\n",
    "    t_lr = widgets.IntSlider(description=\"iter LR\", min=0, max=1, step=1, value=0, continuous_update=False)\n",
    "    t_em = widgets.IntSlider(description=\"iter EM\", min=0, max=1, step=1, value=0, continuous_update=False)\n",
    "\n",
    "    out = widgets.Output()\n",
    "\n",
    "    # Internal state\n",
    "    state = {\n",
    "        \"seed\": int(dataset_kwargs.get(\"seed\", 0)),\n",
    "        \"logreg_out\": None,\n",
    "        \"gmm_out\": None,\n",
    "    }\n",
    "\n",
    "    def recompute():\n",
    "        # Update seeds deterministically (so each click changes dataset)\n",
    "        state[\"seed\"] += 1\n",
    "        ds_kwargs = dict(dataset_kwargs)\n",
    "        ds_kwargs[\"seed\"] = state[\"seed\"]\n",
    "\n",
    "        # Generate data\n",
    "        X1, X2 = create_dataset(**ds_kwargs)\n",
    "\n",
    "        # Train discriminative model (uses YOUR earlier function)\n",
    "        w_path, b_path, loss_path = logistic_regression_gd_path(X1, X2, **logreg_kwargs)\n",
    "        logreg_out = {\"X1\": X1, \"X2\": X2, \"w_path\": w_path, \"b_path\": b_path, \"loss_path\": loss_path}\n",
    "\n",
    "        # Train generative model with EM history\n",
    "        params, samples, r = gmm2_em_and_sample_history(X1, X2, **gmm_kwargs)\n",
    "        gmm_out = {\"X1\": X1, \"X2\": X2, \"params\": params, \"samples\": samples, \"responsibilities\": r}\n",
    "\n",
    "        state[\"logreg_out\"] = logreg_out\n",
    "        state[\"gmm_out\"] = gmm_out\n",
    "\n",
    "        # Update slider ranges\n",
    "        t_lr.max = w_path.shape[0] - 1\n",
    "        t_lr.value = min(t_lr.value, t_lr.max)\n",
    "\n",
    "        # For EM, history length defines slider max\n",
    "        T_em = params[\"history\"][\"pi_path\"].shape[0]\n",
    "        t_em.max = T_em - 1\n",
    "        t_em.value = min(t_em.value, t_em.max)\n",
    "\n",
    "        draw()\n",
    "\n",
    "    def draw():\n",
    "        if state[\"logreg_out\"] is None or state[\"gmm_out\"] is None:\n",
    "            return\n",
    "\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "            # Left: discriminative snapshot\n",
    "            plot_logreg_snapshot(state[\"logreg_out\"], t=int(t_lr.value), ax=axes[0], dims=dims, title=True)\n",
    "\n",
    "            # Right: generative snapshot (uses params history, so ellipses move with t_em)\n",
    "            plot_gmm_snapshot(state[\"gmm_out\"], t=int(t_em.value), ax=axes[1], dims=dims, title=True, show_samples=True)\n",
    "\n",
    "            if share_limits:\n",
    "                X1 = state[\"logreg_out\"][\"X1\"][:, list(dims)]\n",
    "                X2 = state[\"logreg_out\"][\"X2\"][:, list(dims)]\n",
    "                Xall = np.vstack([X1, X2])\n",
    "                xmin, ymin = Xall.min(axis=0)\n",
    "                xmax, ymax = Xall.max(axis=0)\n",
    "                dx, dy = xmax - xmin, ymax - ymin\n",
    "                pad = 0.08\n",
    "                xmin -= pad * (dx + 1e-12); xmax += pad * (dx + 1e-12)\n",
    "                ymin -= pad * (dy + 1e-12); ymax += pad * (dy + 1e-12)\n",
    "                axes[0].set_xlim(xmin, xmax); axes[0].set_ylim(ymin, ymax)\n",
    "                axes[1].set_xlim(xmin, xmax); axes[1].set_ylim(ymin, ymax)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # Wire events\n",
    "    def on_click(_):\n",
    "        recompute()\n",
    "\n",
    "    btn.on_click(on_click)\n",
    "\n",
    "    def on_slider_change(_):\n",
    "        draw()\n",
    "\n",
    "    t_lr.observe(on_slider_change, names=\"value\")\n",
    "    t_em.observe(on_slider_change, names=\"value\")\n",
    "\n",
    "    # Initial compute\n",
    "    recompute()\n",
    "\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([btn, t_lr, t_em]),\n",
    "        out\n",
    "    ])\n",
    "\n",
    "    return ui\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Usage (run this line)\n",
    "# -----------------------------\n",
    "ui = build_discriminative_vs_generative_widget()\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221717b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "036e581f",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
